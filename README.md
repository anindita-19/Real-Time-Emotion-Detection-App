# üòä Real-Time Emotion Detection Web Application üé≠

A deep learning-based emotion detection system that recognizes facial emotions in real-time using a webcam. Built with TensorFlow, OpenCV, and Streamlit for an interactive web interface.

![Python Version](https://img.shields.io/badge/Python-3.12-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.18-orange)
![OpenCV](https://img.shields.io/badge/OpenCV-4.8-green)
![Streamlit](https://img.shields.io/badge/Streamlit-1.54-red)
![License](https://img.shields.io/badge/License-MIT-yellow)

---

## üìã Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Dataset Information](#dataset-information)
- [Model Architecture](#model-architecture)
- [Training Results](#training-results)
- [Installation](#installation)
- [Usage](#usage)
- [Limitations](#limitations)
- [Future Improvements](#future-improvements)
- [Tech Stack](#tech-stack)
- [Project Structure](#project-structure)
- [License](#license)

---

## üéØ Overview

This project implements a real-time emotion detection system capable of recognizing five distinct facial emotions: **Angry**, **Happy**, **Neutral**, **Sad**, and **Surprised**. The system uses a Convolutional Neural Network (CNN) trained on grayscale facial images and provides instant emotion predictions through a user-friendly web interface.

**Note**: This is an educational project developed as part of a machine learning course.

### Why This Project?

Emotion detection has numerous practical applications:
- **Mental Health Monitoring**: Track emotional patterns over time
- **Customer Service**: Analyze customer satisfaction in real-time
- **Education**: Monitor student engagement in virtual classrooms
- **Human-Computer Interaction**: Create emotionally-aware applications
- **Market Research**: Gauge emotional responses to products/content

---

## ‚ú® Features

- ‚úÖ **Real-time emotion detection** from webcam feed
- ‚úÖ **5 emotion categories**: Angry, Happy, Neutral, Sad, Surprised
- ‚úÖ **Beautiful, modern UI** with dark gradient background
- ‚úÖ **Color-coded bounding boxes** for each emotion
- ‚úÖ **Confidence scores** displayed as percentages
- ‚úÖ **Customizable settings** via sidebar controls
- ‚úÖ **Cross-platform support** (Windows, Mac, Linux)

---

## üìä Dataset Information

### Dataset Source
Custom dataset provided by course instructor for educational purposes (not included in repository).

### Dataset Statistics

| Emotion | Number of Images | Percentage |
|---------|-----------------|------------|
| **Angry** | 3,009 | 18.7% |
| **Happy** | 4,097 | 25.5% |
| **Neutral** | 3,026 | 18.8% |
| **Sad** | 3,026 | 18.8% |
| **Surprised** | 2,924 | 18.2% |
| **TOTAL** | **16,082** | **100%** |

### Dataset Characteristics
- **Image Format**: Grayscale (1 channel)
- **Image Size**: 48√ó48 pixels
- **Color Space**: Grayscale
- **Train/Test Split**: 80/20 ratio
  - Training samples: 12,865 images
  - Testing samples: 3,217 images
- **Preprocessing**: Images normalized to [0, 1] range

### Data Distribution

The dataset shows a relatively balanced distribution across emotions, with **Happy** being slightly overrepresented (25.5%) and **Surprised** being slightly underrepresented (18.2%).

**Note**: The dataset is NOT included in this repository due to:
- Large file size (prohibitive for Git/GitHub)
- Copyright/licensing considerations
- Course-provided data (not publicly distributable)

---

## üèóÔ∏è Model Architecture

### Network Design

A custom **Convolutional Neural Network (CNN)** built with Keras Sequential API:

```
Model: Sequential CNN
_________________________________________________________________
Layer (type)                Output Shape              Params
=================================================================
Conv2D (3x3, 45 filters)    (None, 46, 46, 45)        450
MaxPooling2D (2x2)          (None, 23, 23, 45)        0
_________________________________________________________________
Conv2D (3x3, 65 filters)    (None, 21, 21, 65)        26,390
MaxPooling2D (2x2)          (None, 10, 10, 65)        0
_________________________________________________________________
Conv2D (3x3, 128 filters)   (None, 8, 8, 128)         74,880
MaxPooling2D (2x2)          (None, 4, 4, 128)         0
_________________________________________________________________
Flatten                     (None, 2048)              0
_________________________________________________________________
Dense (256 units, ReLU)     (None, 256)               524,544
_________________________________________________________________
Dense (5 units, Softmax)    (None, 5)                 1,285
=================================================================
Total params: 627,549
Trainable params: 627,549
Non-trainable params: 0
```

### Architecture Details

**Convolutional Layers:**
- **Layer 1**: 45 filters (3√ó3), ReLU activation, MaxPooling (2√ó2)
- **Layer 2**: 65 filters (3√ó3), ReLU activation, MaxPooling (2√ó2)
- **Layer 3**: 128 filters (3√ó3), ReLU activation, MaxPooling (2√ó2)

**Dense Layers:**
- **Hidden Layer**: 256 neurons, ReLU activation
- **Output Layer**: 5 neurons, Softmax activation (probability distribution)

### Training Configuration

- **Optimizer**: Adam (learning_rate=0.001)
- **Loss Function**: Categorical Crossentropy
- **Metrics**: Accuracy
- **Batch Size**: 32 (default)
- **Epochs**: 30
- **Training Environment**: Google Colab (CPU)
- **Training Duration**: ~30 minutes

---

## üìà Training Results

### Performance Metrics

| Metric | Training Set | Validation Set |
|--------|-------------|----------------|
| **Accuracy** | 98.57% | 61.08% |
| **Loss** | 0.0478 | 3.2893 |

### Training Progress (30 Epochs)

| Epoch Range | Train Accuracy | Val Accuracy | Train Loss | Val Loss |
|-------------|---------------|--------------|------------|----------|
| 1-5 | 31% ‚Üí 62% | 47% ‚Üí 58% | 1.524 ‚Üí 0.954 | 1.297 ‚Üí 1.092 |
| 6-10 | 63% ‚Üí 80% | 62% ‚Üí 62% | 0.944 ‚Üí 0.537 | 1.001 ‚Üí 1.112 |
| 11-15 | 82% ‚Üí 88% | 62% ‚Üí 61% | 0.474 ‚Üí 0.363 | 1.235 ‚Üí 1.724 |
| 16-20 | 95% ‚Üí 98% | 60% ‚Üí 61% | 0.146 ‚Üí 0.076 | 1.855 ‚Üí 2.398 |
| 21-25 | 97% ‚Üí 98% | 61% ‚Üí 60% | 0.082 ‚Üí 0.055 | 2.592 ‚Üí 3.051 |
| 26-30 | 98% ‚Üí 99% | 59% ‚Üí 61% | 0.063 ‚Üí 0.048 | 3.083 ‚Üí 3.289 |

### Key Observations

1. **Overfitting Present**: Large gap between training (98.57%) and validation (61.08%) accuracy
2. **Early Plateau**: Validation accuracy plateaus around epoch 6-7 at ~61-62%
3. **Training Loss Continues Decreasing**: Model keeps memorizing training data
4. **Validation Loss Increases**: Clear sign of overfitting after epoch 10

### Real-World Performance by Emotion

| Emotion | Performance | Notes |
|---------|------------|-------|
| **Happy** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent | Most accurately predicted |
| **Neutral** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent | High accuracy, consistent |
| **Surprised** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent | Reliable predictions |
| **Angry** | ‚≠ê‚≠ê‚≠ê Moderate | Struggles with detection |
| **Sad** | ‚≠ê‚≠ê‚≠ê Moderate | Often confused with Neutral |

---

## üöÄ Installation

### Prerequisites

- Python 3.8+ (tested on 3.12.9)
- pip (Python package installer)
- Webcam (built-in or external)
- Internet connection (for initial setup)

### Quick Start

1. **Clone the repository**
```bash
git clone https://github.com/anindita-19/emotion-detection-app.git
cd emotion-detection-app
```

2. **Create virtual environment**
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Mac/Linux
python3 -m venv venv
source venv/bin/activate
```

3. **Install dependencies**
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

4. **Verify installation**
```bash
streamlit --version
python -c "import tensorflow as tf; print(f'TensorFlow: {tf.__version__}')"
```

5. **Run the application**
```bash
streamlit run web_app.py
```

The app will automatically open in your browser at `http://localhost:8501`

---

## üíª Usage

### Starting the App

1. Activate your virtual environment
2. Run `streamlit run web_app.py`
3. Click **START** button to activate webcam
4. Allow camera permissions when prompted
5. Position your face in the frame

### Sidebar Controls

- ‚úÖ **Show Bounding Box**: Toggle face detection rectangles
- ‚úÖ **Show Confidence Score**: Display prediction percentages

### Tips for Best Results

- üí° Ensure good, even lighting
- üí° Face the camera directly
- üí° Stay within 2-4 feet from camera
- üí° Avoid cluttered backgrounds
- üí° Make clear facial expressions

---

## ‚ö†Ô∏è Limitations

### 1. Significant Overfitting üî¥

**Problem**: Model memorizes training data instead of learning generalizable patterns.

- Training Accuracy: 98.57%
- Validation Accuracy: 61.08%
- **Gap**: ~37.5 percentage points

**Causes**:
- No data augmentation used
- No regularization (dropout, L2)
- Limited dataset size (16,082 images)
- Model may be too complex for dataset

### 2. Emotion-Specific Challenges

**Struggles with**:
- **Angry**: Often misclassified
- **Sad**: Frequently confused with Neutral

**Performs well with**:
- **Happy**: Clear smile detection
- **Neutral**: Consistent recognition
- **Surprised**: Distinctive wide-eyed expressions

### 3. Environmental Sensitivity

- Poor lighting affects face detection
- Harsh shadows distort features
- Low light may prevent face detection entirely

### 4. Limited Scope

- Only 5 emotion categories (missing: Fear, Disgust, etc.)
- Single face optimization (may struggle with multiple faces)
- Trained on static images (video may differ)

---

## üîß Future Improvements

### High Priority

1. **Address Overfitting**
   - Implement data augmentation (rotation, flip, brightness)
   - Add Dropout layers (0.3-0.5 rate)
   - Apply L2 regularization
   - Use Early Stopping
   - Implement Batch Normalization

2. **Improve Lighting Robustness**
   - Apply histogram equalization
   - Use CLAHE preprocessing
   - Train with varied lighting conditions

3. **Expand Dataset**
   - Collect more diverse training data
   - Include varied demographics
   - Add challenging conditions (glasses, facial hair)

### Medium Priority

4. **Architecture Improvements**
   - Experiment with transfer learning (VGG16, ResNet, MobileNet)
   - Try different layer configurations
   - Compare optimizers (Adam vs SGD vs RMSprop)

5. **Better Face Detection**
   - Replace Haar Cascade with MTCNN or Dlib
   - Add facial landmark detection

6. **More Emotions**
   - Add Fear, Disgust, Contempt
   - Train on larger public datasets (FER2013, AffectNet)

### Low Priority

7. **UI/UX Enhancements**
   - Emotion history tracking over time
   - Export statistics to CSV
   - Dark/light theme toggle
   - Screenshot/recording functionality

---

## üõ†Ô∏è Tech Stack

| Technology | Version | Purpose |
|------------|---------|---------|
| **Python** | 3.12.9 | Programming language |
| **TensorFlow/Keras** | 2.18.0 | Deep learning framework |
| **OpenCV** | 4.8.1.78 | Computer vision & face detection |
| **NumPy** | 1.26.4 | Numerical computing |
| **Streamlit** | 1.54.0 | Web application framework |
| **streamlit-webrtc** | 0.47.1 | Real-time video streaming |

### Supporting Libraries
- **scikit-learn**: Data preprocessing, train/test split, label encoding
- **pickle**: Data serialization
- **aiortc**: WebRTC implementation
- **av**: Audio/video processing

---

## üìÅ Project Structure

```
emotion-detection-app/
‚îÇ
‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îî‚îÄ‚îÄ emotion_detection_model.h5      # Trained model (627K parameters)
‚îÇ
‚îú‚îÄ‚îÄ haarcascade/                         # Optional (falls back to OpenCV)
‚îÇ   ‚îî‚îÄ‚îÄ haarcascade_frontalface_default (1).xml
‚îÇ
‚îú‚îÄ‚îÄ web_app.py                           # Main Streamlit application
‚îú‚îÄ‚îÄ requirements.txt                     # Python dependencies
‚îú‚îÄ‚îÄ .gitignore                           # Git ignore rules
‚îú‚îÄ‚îÄ README.md                            # This file
‚îÇ
‚îî‚îÄ‚îÄ venv/                                # Virtual environment (not in Git)

NOT INCLUDED (Training files):
‚îú‚îÄ‚îÄ CTTC_PROJECT.ipynb                   # Model training notebook
‚îú‚îÄ‚îÄ data/                                # Pickled data (not in repo)
‚îÇ   ‚îú‚îÄ‚îÄ images.p
‚îÇ   ‚îî‚îÄ‚îÄ labels.p
‚îî‚îÄ‚îÄ Emotion/                             # Raw dataset (not in repo)
    ‚îú‚îÄ‚îÄ Angry/
    ‚îú‚îÄ‚îÄ Happy/
    ‚îú‚îÄ‚îÄ Neutral/
    ‚îú‚îÄ‚îÄ Sad/
    ‚îî‚îÄ‚îÄ Surprised/
```

**Note**: The training data, pickle files, and raw dataset are NOT included in this repository due to size and licensing constraints.

---

## üôè Acknowledgments

- **Dataset**: Provided by course instructor for educational purposes
- **TensorFlow Team**: Deep learning framework
- **OpenCV Team**: Computer vision tools
- **Streamlit Team**: Web application framework
- **Google Colab**: Free GPU/CPU training resources

---

## üìÑ License

This project is licensed under the **MIT License** - see below for details.

```
MIT License

Copyright (c) 2026 Anindita

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

## üìß Contact

- **GitHub**: [@anindita-19](https://github.com/anindita-19)

### Contributing

Contributions are welcome! To contribute:

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Commit your changes: `git commit -m "Add feature"`
4. Push to branch: `git push origin feature-name`
5. Open a Pull Request

### Reporting Issues

Found a bug? Please open an issue on GitHub with:
- Clear description of the problem
- Steps to reproduce
- Expected vs. actual behavior
- Screenshots (if applicable)

---

## üéì Educational Purpose

This project was developed as part of a machine learning/computer science course to demonstrate:
- Deep Learning fundamentals
- Convolutional Neural Networks
- Computer Vision techniques
- Model deployment with web frameworks

**Academic Integrity**: This project is shared for educational purposes. If using for academic submissions, please follow your institution's academic honesty policies.

---

## üåü Support

If you found this project helpful or interesting, please give it a ‚≠ê on GitHub!

---

<div align="center">

**Made with ‚ù§Ô∏è for learning and exploration**

**Last Updated**: February 2026

[‚¨Ü Back to Top](#-real-time-emotion-detection-web-application-)

</div>